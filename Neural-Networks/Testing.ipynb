{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass # for neural network construction\n",
    "import pickle                     # to import MNIST data\n",
    "import gzip                       # to import MNIST data\n",
    "import random                     # to initialize weights and biases\n",
    "import numpy as np                # for all needed math\n",
    "from PIL import Image, ImageOps   # for image file processing\n",
    "from time import time             # for performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_derivative(output_activations, y):\n",
    "    return (output_activations - y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Network:\n",
    "    num_layers: int\n",
    "    biases: list\n",
    "    weights: list\n",
    "\n",
    "def init_network(layers):\n",
    "    \n",
    "    return Network(\n",
    "        len(layers),\n",
    "        \n",
    "        # input layer doesn't have biases\n",
    "        [np.random.randn(y, 1) for y in layers[1:]],\n",
    "        \n",
    "        # there are no (weighted) connections into input layer or out of the output layer\n",
    "        [np.random.randn(y, x) for x, y in zip(layers[:-1], layers[1:])]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(nn, a):\n",
    "    for b, w in zip(nn.biases, nn.weights):\n",
    "        a = sigmoid(np.dot(w, a) + b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nn, test_data):\n",
    "    test_results = [(np.argmax(feedforward(nn, x)), y) for (x, y) in test_data]\n",
    "    \n",
    "    return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(nn, training_data, epochs, mini_batch_size, learning_rate, test_data = None):\n",
    "    n = len(training_data)\n",
    "\n",
    "    for j in range(epochs):\n",
    "        random.shuffle(training_data) # that's where \"stochastic\" comes from\n",
    "\n",
    "        mini_batches = [\n",
    "            training_data[k: k + mini_batch_size] for k in range(0, n, mini_batch_size)\n",
    "        ]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            stochastic_gradient_descent(nn, mini_batch, learning_rate) # that's where learning really happes\n",
    "\n",
    "        if test_data:\n",
    "            print('Epoch {0}: accuracy {1}%'.format(f'{j + 1:2}', 100.0 * evaluate(nn, test_data) / len(test_data)))\n",
    "        else:\n",
    "            print('Epoch {0} complete.'.format(f'{j + 1:2}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(nn, mini_batch, eta):\n",
    "    # \"nabla\" is the gradient symbol\n",
    "    nabla_b = [np.zeros(b.shape) for b in nn.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in nn.weights]\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = backprop(nn, x, y) # compute the gradient\n",
    "        \n",
    "        # note that here we call the return values 'delta_nabla', while in the\n",
    "        # backprop function we call them 'nabla'\n",
    "\n",
    "        nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        \n",
    "    nn.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(nn.weights, nabla_w)]\n",
    "    nn.biases  = [b - (eta / len(mini_batch)) * nb for b, nb in zip(nn.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(nn, x, y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in nn.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in nn.weights]\n",
    "\n",
    "    # feedforward\n",
    "    activation = x    # first layer activation is just its input\n",
    "    activations = [x] # list to store all activations, layer by layer\n",
    "    zs = []           # list to store all z vectors, layer by layer\n",
    "\n",
    "    for b, w in zip(nn.biases, nn.weights):\n",
    "        z = np.dot(w, activation) + b  # calculate z for the current layer\n",
    "        zs.append(z)                   # store\n",
    "        activation = sigmoid(z)        # layer output\n",
    "        activations.append(activation) # store\n",
    "\n",
    "    # backward pass\n",
    "\n",
    "    # 1. starting from the output layer\n",
    "    delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) \n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "\n",
    "    # 2. continue back to the input layer (i is the layer index, we're using i instead of l\n",
    "    #    to improve readability -- l looks too much like 1)\n",
    "    for i in range(2, nn.num_layers): # starting from the next-to-last layer\n",
    "        z = zs[-i]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(nn.weights[-i + 1].transpose(), delta) * sp\n",
    "        \n",
    "        nabla_b[-i] = delta\n",
    "        nabla_w[-i] = np.dot(delta, activations[-i - 1].transpose())\n",
    "        \n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"bytes\")\n",
    "    f.close()\n",
    "    \n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [one_hot_encode(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    \n",
    "    return (list(training_data), list(validation_data), list(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    \n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shape(name, data):\n",
    "    print('Shape of {0}: {1}'.format(name, data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network layer 2\n",
      "Shape of weights: (30, 784)\n",
      "Shape of biases: (30, 1)\n",
      "\n",
      "Network layer 3\n",
      "Shape of weights: (10, 30)\n",
      "Shape of biases: (10, 1)\n",
      "\n",
      "Learning process started...\n",
      "\n",
      "Epoch  1: accuracy 89.83%\n",
      "Epoch  2: accuracy 92.15%\n",
      "Epoch  3: accuracy 92.53%\n",
      "Epoch  4: accuracy 93.47%\n",
      "Epoch  5: accuracy 93.54%\n",
      "Epoch  6: accuracy 93.57%\n",
      "Epoch  7: accuracy 94.06%\n",
      "Epoch  8: accuracy 93.8%\n",
      "Epoch  9: accuracy 93.92%\n",
      "Epoch 10: accuracy 94.21%\n",
      "Epoch 11: accuracy 94.41%\n",
      "Epoch 12: accuracy 94.4%\n",
      "Epoch 13: accuracy 94.5%\n",
      "Epoch 14: accuracy 94.7%\n",
      "Epoch 15: accuracy 94.59%\n",
      "\n",
      "Learning process complete in 128 seconds (8.5 seconds per epoch)!\n",
      "\n",
      "Validation (with yet unseen data): accuracy 94.97%\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper() # load data\n",
    "\n",
    "nn = init_network([784, 30, 10])\n",
    "\n",
    "for l in range(0, nn.num_layers - 1):\n",
    "    print('\\nNetwork layer {0}'.format(l + 2)) # disregard the input layer\n",
    "    print_shape('weights', nn.weights[l])\n",
    "    print_shape('biases', nn.biases[l])\n",
    "    \n",
    "# hyper parameters\n",
    "epochs = 15\n",
    "mini_batch_size = 10\n",
    "learning_rate = 3.0\n",
    "    \n",
    "print('\\nLearning process started...\\n')\n",
    "\n",
    "time_start = time()\n",
    "\n",
    "learn(nn, training_data, epochs, mini_batch_size, learning_rate, test_data)\n",
    "\n",
    "time_end = time()\n",
    "\n",
    "time_elapsed = time_end - time_start\n",
    "\n",
    "print('\\nLearning process complete in {0} seconds ({1} seconds per epoch)!\\n'.format(f'{time_elapsed:.0f}', f'{time_elapsed / epochs:.1f}'))\n",
    "\n",
    "print('Validation (with yet unseen data): accuracy {0}%'.format(100.0 * evaluate(nn, validation_data) / len(validation_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_name):\n",
    "    digit = Image.open(file_name)\n",
    "    \n",
    "    # invert, so that background is black (zeros) \n",
    "    digit = ImageOps.invert(digit)\n",
    "    \n",
    "    pixels = digit.load()\n",
    "    \n",
    "    return np.array(digit).reshape((784, 1)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_image(path, file):\n",
    "    x = load_image(path.format(file))\n",
    "\n",
    "    y = feedforward(nn, x)\n",
    "    \n",
    "    bitmap = x.reshape((28, 28))\n",
    "    \n",
    "    file_num = int(file)\n",
    "    result = y.argmax()\n",
    "    \n",
    "    if file_num == result:\n",
    "        ev = 'correctly'\n",
    "    else:\n",
    "        ev = 'incorrectly'\n",
    " \n",
    "    print(file, 'was', ev, 'recognized as', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-MNIST digits:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 was correctly recognized as 0\n",
      "1 was correctly recognized as 1\n",
      "2 was correctly recognized as 2\n",
      "3 was correctly recognized as 3\n",
      "4 was correctly recognized as 4\n",
      "5 was correctly recognized as 5\n",
      "6 was correctly recognized as 6\n",
      "7 was incorrectly recognized as 2\n",
      "8 was correctly recognized as 8\n",
      "9 was correctly recognized as 9\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('Non-MNIST digits:\\n')\n",
    "\n",
    "for file in range(0,10):\n",
    "    recognize_image('./non-MNIST-digits/{0}.png', file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
